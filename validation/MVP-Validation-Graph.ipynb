{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict\n",
    "from pprint import pprint, pformat\n",
    "from itertools import combinations\n",
    "from collections import deque\n",
    "\n",
    "from graph import Graph\n",
    "import networkx as nx\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pandas.options.display.max_colwidth = 0\n",
    "\n",
    "from kf_lib_data_ingest.common.io import (\n",
    "    read_df\n",
    ")\n",
    "from kf_lib_data_ingest.common.concept_schema import (\n",
    "    CONCEPT,\n",
    "    DELIMITER,\n",
    "    concept_from,\n",
    "    concept_attr_from\n",
    ")\n",
    "from kf_lib_data_ingest.common.pandas_utils import merge_wo_duplicates\n",
    "\n",
    "SOURCE_DATA_DIR = os.path.abspath('data')\n",
    "TEST_DATA_DIR = os.path.join(SOURCE_DATA_DIR, 'test')\n",
    "EXTRACT_DATA_DIR = os.path.join('./output/ExtractStage/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0,
     1,
     5,
     26
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted data files ....\n",
      "\n",
      "Test data files ....\n"
     ]
    }
   ],
   "source": [
    "def display_dfs(df_dict):\n",
    "    for key, df in df_dict.items():\n",
    "        print(f'Dataframe {key}')\n",
    "        display(df)\n",
    "        \n",
    "def load_data(input_dir, enable_display=False):\n",
    "    out_dfs = {}\n",
    "    for fn in os.listdir(input_dir):\n",
    "        fp = os.path.join(input_dir, fn)\n",
    "        ext = os.path.splitext(fn)[-1]\n",
    "        if (os.path.isfile(fp) and (not fn.startswith('.')) and (ext not in {'.zip', '.json'})):\n",
    "            out_dfs[fn] = read_df(os.path.join(input_dir, fn))\n",
    "    if enable_display:\n",
    "        print(f'Loading {fn} into df')\n",
    "        display_dfs(out_dfs)\n",
    "\n",
    "    return out_dfs\n",
    "\n",
    "print('\\nExtracted data files ....')\n",
    "# Read in extracted data files into dict\n",
    "# extract_data_dfs = load_data(EXTRACT_DATA_DIR)\n",
    "\n",
    "# Test dataframes\n",
    "print('\\nTest data files ....')\n",
    "test_data_dfs = load_data(TEST_DATA_DIR, enable_display=False)\n",
    "\n",
    "# Dev dataframes\n",
    "df_dict = {\n",
    "    'fp.csv': pandas.DataFrame(\n",
    "        {\n",
    "            CONCEPT.FAMILY.ID: ['F1', 'F2', 'F2'],\n",
    "            CONCEPT.PARTICIPANT.ID: ['P1', 'P1' ,'P2']\n",
    "        }\n",
    "    ),\n",
    "    'fpb.csv': pandas.DataFrame(\n",
    "        {\n",
    "            CONCEPT.FAMILY.ID: ['F1', None],\n",
    "            CONCEPT.PARTICIPANT.ID: ['P3', 'P4'],\n",
    "            CONCEPT.BIOSPECIMEN.ID: ['B3', 'B3'],\n",
    "        }\n",
    "    ),\n",
    "    'fb.csv': pandas.DataFrame(\n",
    "        {\n",
    "            CONCEPT.FAMILY.ID: ['F1', 'F1', 'F1'],\n",
    "            CONCEPT.BIOSPECIMEN.ID: ['B1', 'B1', 'B3']\n",
    "        }\n",
    "    ),\n",
    "     'pg.csv': pandas.DataFrame(\n",
    "        {\n",
    "            CONCEPT.PARTICIPANT.ID: ['P1'],\n",
    "            CONCEPT.SEQ_MANIFEST_FILE.ID: ['G1']\n",
    "        }\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0,
     9,
     16,
     23,
     30,
     84,
     99,
     120,
     131,
     164,
     183,
     192,
     195,
     199,
     222,
     255,
     264,
     265,
     272,
     318,
     335,
     346,
     354,
     417,
     431,
     434,
     435,
     450,
     457,
     520,
     542,
     573,
     607
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building graph from 6 files\n",
      "Inserting df pf.csv into graph\n",
      "Inserting df sfp2.csv into graph\n",
      "Inserting df sfp2.csv into graph\n",
      "Inserting df sg.csv into graph\n",
      "Inserting df sp.csv into graph\n",
      "Inserting df spf.csv into graph\n",
      "Inserting df spf.csv into graph\n",
      "Inserting df pg.csv into graph\n",
      "Inserting df sfp2.csv into graph\n",
      "Inserting df spf.csv into graph\n",
      "Running PARTICIPANT|ID-->FAMILY|ID test on 6 files ...\n",
      "Running FAMILY|ID-->PARTICIPANT|ID test on 6 files ...\n",
      "Running BIOSPECIMEN|ID-->PARTICIPANT|ID test on 6 files ...\n",
      "Running PARTICIPANT|ID-->BIOSPECIMEN|ID test on 6 files ...\n",
      "Running GENOMIC_FILE|URL_LIST-->BIOSPECIMEN|ID test on 6 files ...\n",
      "Running PARTICIPANT|ID-->PARTICIPANT|GENDER test on 6 files ...\n",
      "Running PARTICIPANT|ID-UNIQUE-COUNT test on 6 files ...\n",
      "Running BIOSPECIMEN|ID-UNIQUE-COUNT test on 6 files ...\n",
      "Index(['result', 'name', 'details'], dtype='object')\n",
      "\n",
      "Generated validation report at /Users/singhn4/Projects/kids_first/kf-ingest-packages/kf_ingest_packages/packages/SD_46SK55A3/validation_report.md\n"
     ]
    }
   ],
   "source": [
    "type_relations = {\n",
    "    CONCEPT.FAMILY.ID: {\n",
    "        CONCEPT.PARTICIPANT.ID\n",
    "    },\n",
    "    CONCEPT.PARTICIPANT.ID: {CONCEPT.BIOSPECIMEN.ID},\n",
    "    CONCEPT.BIOSPECIMEN.ID: {CONCEPT.GENOMIC_FILE.URL_LIST},\n",
    "    CONCEPT.GENOMIC_FILE.URL_LIST: {}\n",
    "}\n",
    "\n",
    "def validate_count(count, min_count, max_count):\n",
    "    if count < min_count:\n",
    "        return False\n",
    "    if (max_count is not None) and (count > max_count):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def concept_from(key_or_id):\n",
    "    try:\n",
    "        c = ''.join(key_or_id.split(DELIMITER)[0])\n",
    "    except:\n",
    "        c = None\n",
    "    return c\n",
    "\n",
    "def concept_attr_from(key_or_id):\n",
    "    try:\n",
    "        c = DELIMITER.join(key_or_id.split(DELIMITER)[0:2])\n",
    "    except:\n",
    "        c = None\n",
    "    return c\n",
    "\n",
    "class ExtendedDiGraph(nx.DiGraph):\n",
    "    \"\"\"\n",
    "    Extends graph_theory.Graph with some more functions\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def has_edge(self, left, right, bidirect=True):\n",
    "        return (\n",
    "            super().has_edge(left, right) or\n",
    "            super().has_edge(right, left)\n",
    "        )\n",
    "    \n",
    "    def add_edge(self, left, right, bidirect=True, allow_redundant_path=True): \n",
    "        path_exists = False\n",
    "        # Only add edge if path doesn't already exist between left, right        \n",
    "        if not allow_redundant_path:\n",
    "            try:\n",
    "                path_exists = nx.has_path(self, left, right)\n",
    "            except nx.NodeNotFound:\n",
    "                pass\n",
    "        \n",
    "        if not path_exists:    \n",
    "            super().add_edge(left, right)\n",
    "            if bidirect:\n",
    "                super().add_edge(right, left)\n",
    "        \n",
    "    def parents(self, node_id):\n",
    "        return [edge[0] for edge in self.in_edges(node_id)]\n",
    "    \n",
    "    def children(self, node_id):\n",
    "        return [edge[-1] for edge in self.out_edges(node_id)]\n",
    "    \n",
    "    def debug_view(self):\n",
    "        return {\n",
    "            nid.split(DELIMITER)[-1]: {n.split(DELIMITER)[-1] for n in neighbors}\n",
    "            for nid, neighbors in nx.to_dict_of_dicts(self).items()\n",
    "        }\n",
    "    \n",
    "    def draw(self, **kwargs):\n",
    "        g = nx.DiGraph(self.debug_view())\n",
    "        pos = nx.spring_layout(g)\n",
    "        plt.figure(1,figsize=(12,12)) \n",
    "        kwargs.update(\n",
    "            {\n",
    "                'font-weight': 'bold',\n",
    "                'with_labels': True,\n",
    "                'node_color': '#33ccff',\n",
    "                'node_size': 400\n",
    "            },\n",
    "        )\n",
    "        nx.draw(g, **kwargs)\n",
    "        plt.show()\n",
    "    \n",
    "class TypeGraph(object):\n",
    "    def __init__(self, relations):\n",
    "        self.adj_list = relations\n",
    "        self.graph = self._build()\n",
    "    \n",
    "    def _build(self):\n",
    "        graph = ExtendedDiGraph()\n",
    "        for node, neighbors in self.adj_list.items():\n",
    "            for n in neighbors:\n",
    "                graph.add_edge(node, n)\n",
    "        return graph\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return pformat(nx.to_dict_of_dicts(self.graph))\n",
    "    \n",
    "class Node(object):\n",
    "    def __init__(self, key, value, filepaths=None):\n",
    "        self.id = f'{key}{DELIMITER}{value}'\n",
    "        self.key = key\n",
    "        self.value = value\n",
    "        self.filepaths = filepaths\n",
    "        self.concept = concept_from(key)\n",
    "        self.attribute = concept_attr_from(key)\n",
    "            \n",
    "    def is_identifier(self):\n",
    "        return self.attribute == 'ID'\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return self.id\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return other.id == self.id\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.id\n",
    "    \n",
    "class NodeIndex(object):\n",
    "    def __init__(self):\n",
    "        self.nodes = defaultdict(dict)\n",
    "        \n",
    "    def add_node(self, node):\n",
    "        self.nodes[node.concept][node.id] = node\n",
    "    \n",
    "    def get_node(self, node_id):\n",
    "        concept = concept_from(node_id)\n",
    "        return self.nodes.get(concept, {}).get(node_id)\n",
    "\n",
    "class DataGraph(object):\n",
    "    \n",
    "    def __init__(self, df_dict, type_relations):\n",
    "        self.df_dict = df_dict\n",
    "        self.node_index = NodeIndex()\n",
    "        self.type_graph = TypeGraph(type_relations)\n",
    "        self.build(df_dict)\n",
    "    \n",
    "    def build(self, df_dict):\n",
    "        self.graph = ExtendedDiGraph()\n",
    "        # Build graph of unambiguous connections first\n",
    "        process_later_dfs = {}\n",
    "        print(f'Building graph from {len(df_dict.keys())} files')\n",
    "        for filepath, df in df_dict.items():\n",
    "            process_later = set()\n",
    "            for left, right in combinations(df.columns, 2):\n",
    "                unambiguous_conn = (\n",
    "                    self.type_graph.graph.has_edge(left, right, bidirect=True) \n",
    "                )\n",
    "                if unambiguous_conn:\n",
    "                    self.insert_df(filepath, df[[left, right]])\n",
    "                else:\n",
    "                    process_later.update([left, right])\n",
    "            \n",
    "            df_later = df[list(process_later)]\n",
    "            if not df_later.empty:\n",
    "                process_later_dfs[filepath] = df_later\n",
    "            \n",
    "        # Add ambiguous connections into the graph        \n",
    "        for filepath, df in process_later_dfs.items():\n",
    "            for left, right in combinations(df.columns, 2):\n",
    "                self.insert_df(filepath, df[[left, right]], allow_redundant_path=False)\n",
    "                    \n",
    "    def insert_df(self, filepath, df, allow_redundant_path=True):\n",
    "        print(f'Inserting df {filepath} into graph')\n",
    "        for left, right in combinations(df.columns, 2):\n",
    "            for i, row in df.iterrows():\n",
    "                # Add left and right nodes of edge\n",
    "                nodes = []\n",
    "                for n in [left, right]:\n",
    "                    if row[n] and pandas.notnull(row[n]):\n",
    "                        node = Node(n, row[n], filepaths=set([filepath]))\n",
    "                        self.add_node(node)\n",
    "                        nodes.append(node)\n",
    "                # Add edges\n",
    "                if len(nodes) > 1: # prevents edges with a node = Null\n",
    "                    self.graph.add_edge(\n",
    "                        nodes[0].id, nodes[1].id, \n",
    "                        bidirect=True, \n",
    "                        allow_redundant_path=allow_redundant_path\n",
    "                    )          \n",
    "                    \n",
    "    def add_node(self, node):\n",
    "        if node.id in self.graph:\n",
    "            filepaths = node.filepaths\n",
    "            node = self.get_node(node.id)\n",
    "            node.filepaths.update(filepaths)\n",
    "\n",
    "        self.graph.add_node(node.id)\n",
    "        self.node_index.add_node(node)\n",
    "    \n",
    "    def get_node(self, node_id):\n",
    "        return self.node_index.get_node(node_id)\n",
    "        \n",
    "    def nodes_by_type(self, node_id):\n",
    "        concept = concept_from(node_id)\n",
    "        return self.node_index.nodes.get(concept, {})    \n",
    "    \n",
    "    def connected_nodes(self, node_id, concept_attr):\n",
    "        # -- Search the graph of node_id for nodes of same type --\n",
    "        ret_nodes = set()\n",
    "        visited = set([node_id])\n",
    "        queue = deque([node_id])\n",
    "\n",
    "        # BFS\n",
    "        while queue:\n",
    "            current = queue.popleft()\n",
    "\n",
    "            # Found a node with target concept, add to output\n",
    "            if concept_attr and (concept_attr_from(current) == concept_attr):\n",
    "                ret_nodes.add(current)\n",
    "\n",
    "            # Not found - keep searching neighbors\n",
    "            for neighbor in nx.neighbors(self.graph, current):\n",
    "                # Add neighbor to list if it has not been searched yet\n",
    "                if neighbor not in visited:\n",
    "                    queue.append(neighbor)\n",
    "                    visited.add(neighbor)\n",
    "\n",
    "        return ret_nodes\n",
    "\n",
    "def stack_dfs(df_dict, left, right=None):\n",
    "    cumulative_df = None\n",
    "    for fn, df in df_dict.items():\n",
    "        # Strip whitespace from column names\n",
    "        df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "\n",
    "        # Select subset\n",
    "        if left not in df.columns:\n",
    "            continue\n",
    "        extract_cols = set([left, right]) & set(df.columns)\n",
    "        df = df[extract_cols]\n",
    "\n",
    "        # Re-order columns for debugging\n",
    "        if len(df.columns) > 1:\n",
    "            df = df[[left, right]]\n",
    "        \n",
    "        # Drop rows where value of left col is null    \n",
    "        df = df[(df[left] != '') & (df[left].notnull())]\n",
    "        \n",
    "        # Stack dfs\n",
    "        if cumulative_df is None:\n",
    "            cumulative_df = df\n",
    "\n",
    "        cumulative_df = pandas.concat([cumulative_df, df])\n",
    "    \n",
    "    # Drop duplicates\n",
    "    try:\n",
    "        cumulative_df.drop_duplicates(inplace=True)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "    return cumulative_df\n",
    "\n",
    "def result_to_emoji(result):\n",
    "    result = int(result)\n",
    "    emap = {\n",
    "        0: '❌',\n",
    "        1: '✅',\n",
    "        2: '☑️'\n",
    "    }\n",
    "    return emap[result]\n",
    "\n",
    "class ValidationTest(ABC):\n",
    "    def __init__(self, name, description, test_type, test_params, id=None):\n",
    "        self.id = id\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.test_type = test_type\n",
    "        self.test_params = test_params\n",
    "    \n",
    "    def run(self, df_dict, *args, **kwargs):\n",
    "        print(f'Running {self.id} test on {len(df_dict.values())} files ...')\n",
    "        return self._run(df_dict, *args, **kwargs)\n",
    "        \n",
    "    def build_report(self, *args, **kwargs):\n",
    "        required_columns = set(['name', 'description', 'result', 'details'])\n",
    "\n",
    "        df = self._build_report(*args, **kwargs)\n",
    "        if not isinstance(df, pandas.DataFrame):\n",
    "            raise Exception(\n",
    "                f'{type(self).__name__}._build_report must return '\n",
    "                f'a pandas.DataFrame object!'\n",
    "            )\n",
    "\n",
    "        if not (required_columns.issubset(set(df.columns))):\n",
    "            raise Exception(\n",
    "                f'{type(self).__name__}._build_report DataFrame must have all required '\n",
    "                f'columns: {required_columns}'\n",
    "            )\n",
    "\n",
    "        return df\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _run(self, df_dict):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _build_report(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'test_id': self.id,\n",
    "            'type': self.test_type,\n",
    "            'name': self.name,\n",
    "            'description': self.description,\n",
    "            'input_params': self.test_params\n",
    "        }\n",
    "\n",
    "    def to_dataframe(self):\n",
    "        return pandas.DataFrame([self.to_dict()])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return pformat(self.to_dict())\n",
    "\n",
    "class ValidationTestResult(ABC):\n",
    "    def __init__(\n",
    "        self, validation_test, result, data=None, message=None, result_details=None, error_locations=None\n",
    "    ):\n",
    "        self.id = None\n",
    "        self.validation_test = validation_test\n",
    "        self.result = result\n",
    "        self.data = data\n",
    "        self.message = message or (\n",
    "            f\"{result_to_emoji(int(result))} \"\n",
    "            f\"{self.validation_test.test_type.title()} \"\n",
    "            f\"for '{self.validation_test.name}' \"\n",
    "            f\"{'Succeeded!' if result else 'Failed!'}\"\n",
    "        )\n",
    "        self.result_details = result_details\n",
    "        self.error_locations = error_locations\n",
    "    \n",
    "    def to_dict(self):\n",
    "        d = self.validation_test.to_dict()\n",
    "        d.update({\n",
    "            'result': self.result,\n",
    "            'data': self.data,\n",
    "            'message': self.message,\n",
    "            'details': self.result_details,\n",
    "            'error_locations': self.error_locations\n",
    "        })\n",
    "        return d\n",
    "    \n",
    "    def to_dataframe(self):\n",
    "        d = self.to_dict()\n",
    "        df = pandas.DataFrame([d])\n",
    "        return df.drop('data', axis=1)\n",
    "      \n",
    "    def __repr__(self):\n",
    "        return pformat(self.to_dict())\n",
    "        \n",
    "class CountValidationTest(ValidationTest):\n",
    "    def __init__(self, name, description, concept_name, min_count, max_count):\n",
    "        super().__init__(\n",
    "            name,\n",
    "            description,\n",
    "            id=f'{concept_name}-UNIQUE-COUNT', \n",
    "            test_type='count-test',\n",
    "            test_params=(concept_name, min_count, max_count)\n",
    "        )\n",
    "        self.concept_name = concept_name\n",
    "        self.min_count = min_count\n",
    "        self.max_count = max_count\n",
    "        self.test_result = None\n",
    "    \n",
    "    def to_dict(self):\n",
    "        d = super().to_dict()\n",
    "        d.update(\n",
    "            {\n",
    "                'concept': self.concept_name,\n",
    "                'min_count': self.min_count,\n",
    "                'max_count': self.max_count\n",
    "            }\n",
    "        )\n",
    "        return d\n",
    "    \n",
    "    def _run(self, df_dict):\n",
    "        # Stack dfs vertically into 1 df         \n",
    "        cumulative_df = stack_dfs(\n",
    "            df_dict,\n",
    "            self.concept_name\n",
    "        )\n",
    "        if (cumulative_df is None) or (cumulative_df.empty):\n",
    "            self.test_result = ValidationTestResult(\n",
    "                self, 2, None, \n",
    "                result_details=(\n",
    "                    'Test did not run due to missing required columns: '\n",
    "                    f'{self.concept_name}'\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            # Unique count test\n",
    "            error = None\n",
    "            uc = cumulative_df.nunique().values[0]\n",
    "            result = validate_count(uc, self.min_count, self.max_count)\n",
    "            # Count check failed\n",
    "            if not result:\n",
    "                error = (self.concept_name, uc)     \n",
    "            self.test_result = ValidationTestResult(self, result, None, result_details=error)\n",
    "        \n",
    "        return self.test_result\n",
    "        \n",
    "\n",
    "    def _build_report(self):\n",
    "        test_df = self.test_result.to_dataframe()\n",
    "        report_df = test_df[['result', 'name', 'description', 'details']]\n",
    "        report_df['result'] = test_df['result'].apply(\n",
    "            lambda x: result_to_emoji(x)\n",
    "        )\n",
    "        report_df['details'] = test_df['details'].apply(\n",
    "            lambda x: f'Found {x[1]} {x[0]} in data' if x else None\n",
    "        )\n",
    "        return report_df\n",
    "\n",
    "class RelationshipValidationTest(ValidationTest):\n",
    "    \"\"\"\n",
    "    Validate relationships between enitity instances\n",
    "\n",
    "    For example given:\n",
    "    \n",
    "        self.node_left=SPECIMEN.ID\n",
    "        self.node_right=PARTICIPANT.ID\n",
    "        self.min_relations=1\n",
    "        self.max_relations =1\n",
    "\n",
    "    A SPECIMEN must be linked to at least 1 PARTICIPANT and no more than 1 PARTICIPANT\n",
    "    Find the SPECIMENS that violate this rule\n",
    "    \"\"\"  \n",
    "    def __init__(\n",
    "        self, name, description, node_left, node_right, min_relations, max_relations, data_graph, \n",
    "        include_implied_conns=True\n",
    "    ):\n",
    "        super().__init__(\n",
    "            name,\n",
    "            description,\n",
    "            id=f'{node_left}-->{node_right}', \n",
    "            test_type='relationship-test',\n",
    "            test_params=(node_left, node_right, min_relations, max_relations)\n",
    "        )\n",
    "        self.node_left = node_left\n",
    "        self.node_right = node_right\n",
    "        self.min_relations = min_relations\n",
    "        self.max_relations = max_relations\n",
    "        self.data_graph = data_graph\n",
    "        self.include_implied_conns=include_implied_conns\n",
    "        self.test_result = None\n",
    "    \n",
    "    def to_dict(self):\n",
    "        d = super().to_dict()\n",
    "        d.update(\n",
    "            {'relation': (self.node_left, self.node_right)}\n",
    "        )\n",
    "        return d\n",
    "    \n",
    "    def _run(self, df_dict):\n",
    "        error_dict = {}\n",
    "        for node in self.data_graph.nodes_by_type(self.node_left):\n",
    "            neighbors = {\n",
    "                n for n in self.data_graph.graph.neighbors(node)\n",
    "                if concept_from(n) == concept_from(self.node_right)\n",
    "            }\n",
    "            if not validate_count(len(neighbors), self.min_relations, self.max_relations):\n",
    "                error_dict[node] = neighbors\n",
    "\n",
    "        if self.include_implied_conns:\n",
    "            nodes = [\n",
    "                node for node, errors in error_dict.items()\n",
    "                if not errors\n",
    "            ]\n",
    "            for node in nodes:\n",
    "                indirect_conn_nodes = self.data_graph.connected_nodes(\n",
    "                    node, self.node_right\n",
    "                )\n",
    "                # Found valid connection through indirect links         \n",
    "                if validate_count(\n",
    "                    len(indirect_conn_nodes), self.min_relations, self.max_relations\n",
    "                ):\n",
    "                    error_dict.pop(node)\n",
    "                else:                \n",
    "                    # Node is still invalid due to indirect links\n",
    "                    error_dict[node] = indirect_conn_nodes\n",
    "        \n",
    "        # Collect file locations of errors\n",
    "        file_locs_of_errors = {\n",
    "            error_node: self.data_graph.get_node(error_node).filepaths\n",
    "            for error_node in error_dict\n",
    "        }\n",
    "        \n",
    "        # Create test result        \n",
    "        self.test_result = ValidationTestResult(\n",
    "            self, \n",
    "            len(error_dict) < 1, \n",
    "            result_details=error_dict,\n",
    "            error_locations=file_locs_of_errors\n",
    "        )\n",
    "        return self.test_result\n",
    "    \n",
    "    def _build_report(self):\n",
    "        def format_errors(error_dict, left, right):\n",
    "            if len(self.data_graph.nodes_by_type(left)) == len(error_dict):\n",
    "                return f'0 {concept_from(left).title()} have linked {right}'\n",
    "\n",
    "            msgs = []\n",
    "            for node_id, errors in error_dict.items():\n",
    "                n = self.data_graph.get_node(node_id)\n",
    "                concept_right = concept_from(right).title()\n",
    "                if len(errors) > 1:\n",
    "                    e_ids = {self.data_graph.get_node(e_id).value\n",
    "                         for e_id in errors}\n",
    "                    error_str = f'{len(e_ids)} {concept_right} entities: {e_ids}'\n",
    "                else:\n",
    "                    error_str = f'0 {concept_right} entities'\n",
    "                    \n",
    "                msgs.append(f'{concept_from(node_id).title()} {n.value} '\n",
    "                            f'is linked to {error_str}')\n",
    "            return '\\n'.join(msgs)\n",
    "                    \n",
    "        def format_error_locs(loc_dict, left):\n",
    "            if len(self.data_graph.nodes_by_type(left)) == len(loc_dict):\n",
    "                return None\n",
    "            \n",
    "            msgs = []\n",
    "            for node_id, locations in loc_dict.items():\n",
    "                n = self.data_graph.get_node(node_id)\n",
    "                msgs.append(f\"Found {n.value} in files: {','.join(locations)}\")\n",
    "            return '\\n'.join(msgs)\n",
    "            \n",
    "        test_df = self.test_result.to_dataframe()\n",
    "        report_df = test_df[['result', 'name', 'description', 'details', 'error_locations']]\n",
    "        report_df['result'] = test_df['result'].apply(lambda x: result_to_emoji(x))\n",
    "        report_df['details'] = test_df['details'].apply(\n",
    "            lambda x: format_errors(x, self.node_left, self.node_right)\n",
    "        )\n",
    "        report_df['error_locations'] = test_df['error_locations'].apply(\n",
    "            lambda x: format_error_locs(x, self.node_left)\n",
    "        )\n",
    "        \n",
    "        return report_df\n",
    "\n",
    "def markdown_report(df):\n",
    "    filepath = os.path.join(os.getcwd(), 'validation_report.md')\n",
    "    \n",
    "    # Prepare data \n",
    "    out_cols = ['result', 'name', 'details', 'error_locations']\n",
    "    counts = (\n",
    "        df[df['test_type'] == 'count-test'][out_cols]\n",
    "        .drop(['error_locations'],axis=1)\n",
    "    )\n",
    "    print(counts.columns)\n",
    "    relations = df[df['test_type'] == 'relationship-test'][out_cols]\n",
    "    attributes = df[df['test_type'] == 'attribute-test'][out_cols]\n",
    "    definitions = df[['test_type', 'name', 'description']]\n",
    "    \n",
    "    # Write markdown     \n",
    "    output = []\n",
    "    output.append('# Validation Report')\n",
    "    output.append('## Counts')\n",
    "    output.append(counts.to_markdown(index=False))\n",
    "    output.append('\\n## Relations')\n",
    "    output.append(relations.to_markdown(index=False))\n",
    "    output.append('\\n## Attributes')\n",
    "    output.append(attributes.to_markdown(index=False))\n",
    "    output.append('\\n## Test Definitions')\n",
    "    output.append(definitions.to_markdown(index=False))\n",
    "    \n",
    "    with open(filepath, 'w') as md_file:\n",
    "        md_file.write('\\n'.join(output))\n",
    "    \n",
    "    print(f'\\nGenerated validation report at {filepath}')\n",
    "\n",
    "def run_tests(df_dict, test_params, type_relations):\n",
    "    report_dfs = []\n",
    "    \n",
    "    # Build data graph\n",
    "    g = DataGraph(df_dict, type_relations)\n",
    "\n",
    "    # Run tests\n",
    "    for test_type, tests in test_params.items():\n",
    "        if test_type == 'count-test':\n",
    "            cls = CountValidationTest\n",
    "            extra_params = ()\n",
    "        else:\n",
    "            cls = RelationshipValidationTest\n",
    "            extra_params = (g,)\n",
    "            \n",
    "        for test in tests:\n",
    "            test_obj = cls(\n",
    "                test['name'], test['desc'], *test['params'], \n",
    "                *extra_params, **test.get('kwargs', {})\n",
    "            )\n",
    "            test_df = test_obj.run(df_dict).to_dataframe()\n",
    "            report_df = test_obj.build_report()            \n",
    "            report_df['test_type'] = test_type\n",
    "            report_dfs.append(report_df)\n",
    "    \n",
    "    # Collect test report dfs     \n",
    "    report_df = pandas.concat(report_dfs)\n",
    "    report_df = report_df.sort_values(by=['result', 'test_type'])\n",
    "    \n",
    "    # Create markdown report\n",
    "    markdown_report(report_df)\n",
    "\n",
    "    return report_df\n",
    "\n",
    "test_params = {\n",
    "    'relationship-test': [\n",
    "        {\n",
    "            'name': 'A Participant is in at least 1 Family Group',\n",
    "            'desc': 'Every uniquely identifiable Participant must be linked to at '\n",
    "                    ' least 1 uniquely identifiable Family within the study',\n",
    "            'params': (CONCEPT.PARTICIPANT.ID, CONCEPT.FAMILY.ID, 1, None)\n",
    "        },\n",
    "        {\n",
    "            'name': 'A Family Group must have at least 1 Participant',\n",
    "            'desc': 'Every uniquely identifiable Family Group must have '\n",
    "                    'at least 1 uniquely identifiable Participant within the study',\n",
    "            'params': (CONCEPT.FAMILY.ID, CONCEPT.PARTICIPANT.ID, 1, None)\n",
    "        },\n",
    "        {\n",
    "            'name': 'A Specimen comes from 1 Participant',\n",
    "            'desc': 'Every uniquely identifiable Specimen must be linked to '\n",
    "                    ' exactly 1 uniquely identifiable Participant within the study',\n",
    "            'params': (CONCEPT.BIOSPECIMEN.ID, CONCEPT.PARTICIPANT.ID, 1, 1)\n",
    "        },\n",
    "        {\n",
    "            'name': 'A Participant must have at least 1 Specimen',\n",
    "            'desc': 'Every uniquely identifiable Participant must have at least 1 '\n",
    "                    ' uniquely identifiable Specimen within the study',\n",
    "            'params': (CONCEPT.PARTICIPANT.ID, CONCEPT.BIOSPECIMEN.ID, 1, None)\n",
    "        },\n",
    "        {\n",
    "            'name': 'A Sequence Manifest File Record represents only 1 Specimen',\n",
    "            'desc': 'Every uniquely identifiable Sequence Manifest File Record must be linked to '\n",
    "                    'exactly 1 uniquely identifiable Specimen within the study',\n",
    "            'params': (CONCEPT.GENOMIC_FILE.URL_LIST, CONCEPT.BIOSPECIMEN.ID, 1, 1),\n",
    "        },\n",
    "#         {\n",
    "#             'name': 'A Specimen must have at least 1 Sequence Manifest File Record',\n",
    "#             'desc': 'Every uniquely identifiable specimen must be linked to '\n",
    "#                     'at least 1 uniquely identifiable Sequence Manifest File Record '\n",
    "#                     'within the study',\n",
    "#             'params': (CONCEPT.BIOSPECIMEN.ID, CONCEPT.GENOMIC_FILE.URL_LIST, 1, None)\n",
    "#         }\n",
    "    ],\n",
    "    'attribute-test': [\n",
    "        {\n",
    "            'name': 'A Participant must have exactly 1 gender',\n",
    "            'desc': 'Every uniquely identifiable Participant must have exactly 1'\n",
    "                    ' gender from the acceptable list: Male, Female',\n",
    "            'params': (CONCEPT.PARTICIPANT.ID, CONCEPT.PARTICIPANT.GENDER, 1, 1)\n",
    "        }\n",
    "    ],\n",
    "    'count-test': [\n",
    "        {\n",
    "            'name': 'Expected Participant Unique Count = 10',\n",
    "            'desc': 'The number of uniquely identifiable participants found in '\n",
    "                    'study must be equal to the expected count',\n",
    "            'params': (CONCEPT.PARTICIPANT.ID, 10, 10)\n",
    "        },\n",
    "        {\n",
    "            'name': 'Expected Specimen Unique Count = 12',\n",
    "            'desc': 'The number of uniquely identifiable specimens found in '\n",
    "                    'study must be equal to the expected count',\n",
    "            'params': (CONCEPT.BIOSPECIMEN.ID, 12, 12)\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "report_df = run_tests(test_data_dfs, test_params, type_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO\n",
    "# - Do validation with graph without indirect conn, remove success nodes from the graph\n",
    "# - Do validation with graph with indirect conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
